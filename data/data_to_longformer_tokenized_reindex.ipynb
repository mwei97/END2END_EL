{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use longformer tokenizer + page id to indice based on 'entity.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenized elq format\n",
    "\n",
    "- 'id': 'doc_id'\n",
    "- 'text': 'text'.join\n",
    "- 'mentions': index span (char) of mentions, list of lists (end idx + 1)\n",
    "- 'tokenized_text_ids': tokenized text id\n",
    "- 'tokenized_mention_idxs': similar to span_position, list of lists (end idx + 1)\n",
    "- 'label_id': id based on 'entity.jsonl'\n",
    "- 'wikidata_id': not include, plan to remove in code\n",
    "- 'entity': 'wiki_titles'\n",
    "- 'label': 'wiki_contexts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_bounds(mentions, full_example, tokenizer):\n",
    "    example_ranges = mentions\n",
    "    \n",
    "    char_in_mention_idx_map = [[] for _ in range(len(full_example))]\n",
    "    all_mention_bounds = []\n",
    "    for m, ment in enumerate(example_ranges):\n",
    "        for c in range(ment[0], ment[1]):\n",
    "            char_in_mention_idx_map[c].append(m)\n",
    "        all_mention_bounds.append(ment[0])\n",
    "        all_mention_bounds.append(ment[1])\n",
    "    all_mention_bounds = [0] + all_mention_bounds + [len(full_example)]\n",
    "    all_mention_bounds = list(set(all_mention_bounds))\n",
    "    all_mention_bounds.sort()\n",
    "    \n",
    "    example_chunks = [full_example[all_mention_bounds[b]:(all_mention_bounds[b+1])] for b in range(len(all_mention_bounds) - 1)]\n",
    "    chunk_idx_to_mention_idx_map = []\n",
    "    bound_idx = 0\n",
    "    for c, chunk in enumerate(example_chunks):\n",
    "        assert bound_idx == all_mention_bounds[c]\n",
    "        try:\n",
    "            chunk_idx_to_mention_idx_map.append(char_in_mention_idx_map[all_mention_bounds[c]])\n",
    "        except:\n",
    "            print(\"error checkpoint\")\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "        bound_idx += len(chunk)\n",
    "    mention_idx_to_chunk_idx_map = {}\n",
    "    chunk_idx_to_tokenized_bounds = {}\n",
    "    mention_idxs = []\n",
    "    all_token_ids = []\n",
    "    cum_len = 0\n",
    "    for c, chunk in enumerate(example_chunks):\n",
    "        #chunk_tokens = tokenizer.encode(chunk)\n",
    "        chunk_tokens = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(chunk))\n",
    "        all_token_ids += chunk_tokens\n",
    "        chunk_bounds = [cum_len, cum_len+len(chunk_tokens)]\n",
    "        for m in chunk_idx_to_mention_idx_map[c]:\n",
    "            if m not in mention_idx_to_chunk_idx_map:\n",
    "                mention_idx_to_chunk_idx_map[m] = chunk_bounds\n",
    "            else:\n",
    "                existing_chunk_bounds = mention_idx_to_chunk_idx_map[m]\n",
    "                mention_idx_to_chunk_idx_map[m] = [\n",
    "                    min(existing_chunk_bounds[0], chunk_bounds[0]),\n",
    "                    max(existing_chunk_bounds[1], chunk_bounds[1]),\n",
    "                ]\n",
    "        cum_len += len(chunk_tokens)\n",
    "    for mention_idx in range(len(mention_idx_to_chunk_idx_map)):\n",
    "        assert mention_idx in mention_idx_to_chunk_idx_map\n",
    "        mention_tokenized_bound = mention_idx_to_chunk_idx_map[mention_idx]\n",
    "        mention_idxs.append(mention_tokenized_bound)\n",
    "    for m in range(len(mention_idxs)):\n",
    "        mention_bounds = mentions[m]\n",
    "        mention_tok_bounds = mention_idxs[m]\n",
    "        tokenized_mention = tokenizer.decode(all_token_ids[\n",
    "            mention_tok_bounds[0]:mention_tok_bounds[1]\n",
    "        ])\n",
    "        #target_mention = full_example[mention_bounds[0]:mention_bounds[1]].lower()\n",
    "        target_mention = full_example[mention_bounds[0]:mention_bounds[1]]\n",
    "        try:\n",
    "            assert tokenized_mention == target_mention\n",
    "        except:\n",
    "            # only keep letters and whitespace\n",
    "            only_letter_tokenized_mention = \"\"\n",
    "            only_letter_target_mention = \"\"\n",
    "            for char in tokenized_mention:\n",
    "                if char in string.ascii_letters:\n",
    "                    only_letter_tokenized_mention += char\n",
    "            for char in target_mention:\n",
    "                if char in string.ascii_letters:\n",
    "                    only_letter_target_mention += char\n",
    "            print(\"{} {}\".format(tokenized_mention, target_mention))\n",
    "            try:\n",
    "                assert only_letter_tokenized_mention.lower() == only_letter_target_mention.lower()\n",
    "            except:\n",
    "                print(only_letter_tokenized_mention, only_letter_target_mention)\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "    return all_token_ids, mention_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_longformer_tokenized(sample):\n",
    "    res = {}\n",
    "    res['id'] = sample['doc_id']\n",
    "    res['text'] = ' '.join(sample['text'])\n",
    "    \n",
    "    # generate char indice 'mentions'\n",
    "    assert len(sample['text'])==len(sample['start_idxs'])==len(sample['end_idxs'])\n",
    "    char_cnt = 0\n",
    "    mentions = []\n",
    "    new_mention = []\n",
    "    open_mention = False\n",
    "    for i, word in enumerate(sample['text']):\n",
    "        if sample['start_idxs'][i]==1:\n",
    "            new_mention.append(char_cnt)\n",
    "        char_cnt += len(word) + 1\n",
    "        if sample['end_idxs'][i]==1:\n",
    "            new_mention.append(char_cnt-1)\n",
    "            mentions.append(new_mention)\n",
    "            new_mention = []\n",
    "    # sanity check\n",
    "    assert len(mentions)==len(sample['mentions'])\n",
    "    for i in range(len(mentions)):\n",
    "        try:\n",
    "            assert res['text'][mentions[i][0]:mentions[i][1]]==sample['mentions'][i]\n",
    "        except:\n",
    "            print(res['text'][mentions[i][0]:mentions[i][1]], ' ', sample['mentions'][i])\n",
    "    res['mentions'] = mentions\n",
    "    \n",
    "    # generate tokenized texts and mention bounds\n",
    "    # code from Belinda: create_all_entity_finetuning.py\n",
    "    all_token_ids, mention_idxs = get_tokenized_bounds(res['mentions'], res['text'], tokenizer)\n",
    "    res['tokenized_text_ids'] = all_token_ids\n",
    "    res['tokenized_mention_idxs'] = mention_idxs\n",
    "    assert len(mention_idxs)==len(sample['wiki_titles'])\n",
    "    \n",
    "    res['label_id'] = sample['wiki_ids']\n",
    "    res['entity'] = sample['wiki_titles']\n",
    "    res['label'] = sample['wiki_contexts']\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 514/946 [00:03<00:02, 185.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep .   Rep.\n",
      "Rep. Rep .\n",
      "Goldman , Sachs & Co   Goldman, Sachs & Co\n",
      "Goldman, Sachs & Co Goldman , Sachs & Co\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 592/946 [00:03<00:02, 169.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wisc .   Wisc.\n",
      "Wisc .   Wisc.\n",
      "Wisc. Wisc .\n",
      "Wisc. Wisc .\n",
      "Washington , D.C.   Washington, D.C.\n",
      "Washington, D.C. Washington , D.C.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 796/946 [00:04<00:00, 190.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colo .   Colo.\n",
      "Colo. Colo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 910/946 [00:05<00:00, 205.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colo .   Colo.\n",
      "Colo. Colo .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [00:05<00:00, 167.93it/s]\n",
      "946it [00:00, 8867.31it/s]\n",
      "100%|██████████| 216/216 [00:01<00:00, 175.88it/s]\n",
      "216it [00:00, 5008.82it/s]\n",
      "100%|██████████| 231/231 [00:01<00:00, 181.11it/s]\n",
      "231it [00:00, 5954.05it/s]\n"
     ]
    }
   ],
   "source": [
    "in_fpath = 'AIDA-YAGO2-en_desc'\n",
    "out_fpath = 'AIDA-YAGO2_longformer/tokenized'\n",
    "\n",
    "fnames = ['train.json', 'dev.json', 'test.json']\n",
    "num_longs = []\n",
    "\n",
    "for fname in fnames:\n",
    "    in_fname = os.path.join(in_fpath, fname)\n",
    "    with open(in_fname) as fin:\n",
    "        orig_data = json.load(fin)\n",
    "    \n",
    "    longformer_tokenized = []\n",
    "    for sample in tqdm(orig_data):\n",
    "        longformer_example = to_longformer_tokenized(sample)\n",
    "        longformer_tokenized.append(longformer_example)\n",
    "    \n",
    "    fname = fname+'l'\n",
    "    out_fname = os.path.join(out_fpath, fname)\n",
    "    \n",
    "    num_long = []\n",
    "    with open(out_fname, 'w') as wf:\n",
    "        for i, example in tqdm(enumerate(longformer_tokenized)):\n",
    "            if len(example['tokenized_text_ids']) > 512:\n",
    "                num_long.append(i)\n",
    "            b = wf.write(json.dumps(example) + \"\\n\")\n",
    "    num_longs.append(num_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/id2title.json') as f:\n",
    "    id2title = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wiki_ents = open(\"models/entity.jsonl\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5903527\n"
     ]
    }
   ],
   "source": [
    "all_wiki_ents = [json.loads(line) for line in all_wiki_ents]\n",
    "print(len(all_wiki_ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title2id = {line['title']: i for i, line in enumerate(all_wiki_ents)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "page2id = {line['idx'].split('=')[-1]: i for i, line in enumerate(all_wiki_ents)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_title_from_api(pageid, client=None):\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&pageids={pageid}&format=json\"\n",
    "\n",
    "    try:\n",
    "        # Package the request, send the request and catch the response: r\n",
    "        r = requests.get(url)\n",
    "\n",
    "        # Decode the JSON data into a dictionary: json_data\n",
    "        json_data = r.json()\n",
    "\n",
    "        if len(json_data[\"query\"][\"pages\"]) > 1:\n",
    "            print(\"WARNING: more than one result returned from wikipedia api\")\n",
    "\n",
    "        for _, v in json_data[\"query\"][\"pages\"].items():\n",
    "            title = v[\"title\"]\n",
    "    except:\n",
    "        pass\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex(fpath, split):\n",
    "    fname = fpath+split\n",
    "    examples = []\n",
    "    filelines = open(fname).readlines()\n",
    "    for line in filelines:\n",
    "        json_line = json.loads(line)\n",
    "        examples.append(json_line)\n",
    "    \n",
    "    for e, example in tqdm(enumerate(examples)):\n",
    "        old_label_id = example['label_id']\n",
    "        entity = example['entity']\n",
    "        \n",
    "        new_label_id = []\n",
    "        for i, old_id in enumerate(old_label_id):\n",
    "            new_id = page2id[str(old_id)]\n",
    "            new_label_id.append(new_id)\n",
    "            try:\n",
    "                assert all_wiki_ents[new_id]['title'] == entity[i]\n",
    "            except:\n",
    "                # try compare with wiki url result\n",
    "                #old_id = int(old_id)\n",
    "                title = id2title.get(str(old_id))\n",
    "                if title is None:\n",
    "                    title = _get_title_from_api(int(old_id))\n",
    "                    id2title[old_id] = title\n",
    "                try:\n",
    "                    assert all_wiki_ents[new_id]['title'] == title or entity[i] == title \n",
    "                except:\n",
    "                    print(e, ' ', example['id'], ' ', old_id, ' ', new_id, ' ', entity[i], ' ', all_wiki_ents[new_id]['title'])\n",
    "#                 else:\n",
    "#                     print(e, ' ', example['id'], ' ', old_id, ' ', new_id, ' ', entity[i], ' ', all_wiki_ents[new_id]['title'])\n",
    "                entity[i] = all_wiki_ents[new_id]['title']\n",
    "        example['label_id'] = new_label_id\n",
    "        example['entity'] = entity\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [00:05, 34.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128   129 Viacom   24580262   2891949   Viacom (1971–2005)   Viacom (original)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:08, 72.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235   236 Promodes   2688005   664924   Les Échos (France)   Les Échos (newspaper)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "251it [00:08, 67.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247   248 RUGBY   1196374   372695   Halifax RLFC   Halifax R.L.F.C.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "260it [00:08, 45.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259   260 SOCCER   10410246   1546285   OKS 1945 Olsztyn   Stomil Olsztyn (football)\n",
      "266   267 SOCCER   1537131   443668   V.C. Eendracht Aalst 2002   SC Eendracht Aalst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "327it [00:10, 31.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322   323 RUGBY   1196374   372695   Halifax RLFC   Halifax R.L.F.C.\n",
      "322   323 RUGBY   1196374   372695   Halifax RLFC   Halifax R.L.F.C.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "340it [00:10, 40.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341   342 SOCCER   10410246   1546285   OKS 1945 Olsztyn   Stomil Olsztyn (football)\n",
      "341   342 SOCCER   10410246   1546285   OKS 1945 Olsztyn   Stomil Olsztyn (football)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "370it [00:11, 26.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365   366 SOCCER   1537131   443668   V.C. Eendracht Aalst 2002   SC Eendracht Aalst\n",
      "365   366 SOCCER   1537131   443668   V.C. Eendracht Aalst 2002   SC Eendracht Aalst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "417it [00:12, 51.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414   415 RUGBY   1196374   372695   Halifax RLFC   Halifax R.L.F.C.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "485it [00:14, 41.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473   474 Senate   403248   171440   Sultan, Crown Prince of Saudi Arabia   Sultan bin Abdulaziz Al Saud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "551it [00:15, 47.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545   546 CYCLING   2354465   604821   Rabobank (cycling team)   Team Jumbo–Visma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "641it [00:16, 77.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639   640 CYCLING   2354465   604821   Rabobank (cycling team)   Team Jumbo–Visma\n",
      "640   641 CYCLING   2354465   604821   Rabobank (cycling team)   Team Jumbo–Visma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "661it [00:17, 47.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "654   655 CRICKET   3182138   746081   Dave Richardson   Dave Richardson (cricketer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "748it [00:18, 65.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726   727 Barrier   1873300   512081   Vale (mining company)   Vale (company)\n",
      "726   727 Barrier   1873300   512081   Vale (mining company)   Vale (company)\n",
      "726   727 Barrier   1873300   512081   Vale (mining company)   Vale (company)\n",
      "726   727 Barrier   1873300   512081   Vale (mining company)   Vale (company)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [00:19, 85.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790   791 PRESS   2286075   591510   Legal Department   Legal Department, Hong Kong\n",
      "801   802 CYCLING   2354465   604821   Rabobank (cycling team)   Team Jumbo–Visma\n",
      "801   802 CYCLING   2354465   604821   Rabobank (cycling team)   Team Jumbo–Visma\n",
      "801   802 CYCLING   2354465   604821   Rabobank (cycling team)   Team Jumbo–Visma\n",
      "801   802 CYCLING   2354465   604821   Rabobank (cycling team)   Team Jumbo–Visma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "825it [00:19, 80.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806   807 LOMBARDI   2354465   604821   Rabobank (cycling team)   Team Jumbo–Visma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "870it [00:19, 115.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875   876 PRESS   4665846   956613   Muslim Commercial Bank   MCB Bank Limited\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "946it [00:20, 45.19it/s] \n",
      "100%|██████████| 946/946 [00:01<00:00, 547.46it/s]\n",
      "135it [00:00, 195.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108   1055testa CYCLING   2354465   604821   Rabobank (cycling team)   Team Jumbo–Visma\n",
      "108   1055testa CYCLING   2354465   604821   Rabobank (cycling team)   Team Jumbo–Visma\n",
      "108   1055testa CYCLING   2354465   604821   Rabobank (cycling team)   Team Jumbo–Visma\n",
      "108   1055testa CYCLING   2354465   604821   Rabobank (cycling team)   Team Jumbo–Visma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "216it [00:00, 253.14it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 5886.86it/s]\n",
      "26it [00:00, 106.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10   1173testb RUGBY   5746768   1087118   Dan Crowley   Dan Crowley (rugby player)\n",
      "12   1175testb SOCCER   2384790   610490   AFC Progresul Bucureşti   AS Progresul București\n",
      "12   1175testb SOCCER   2384790   610490   AFC Progresul Bucureşti   AS Progresul București\n",
      "39   1202testb SOCCER   616593   235776   Luis Enrique Martínez García   Luis Enrique (footballer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [00:00, 155.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n",
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n",
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n",
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n",
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n",
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n",
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n",
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n",
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n",
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n",
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n",
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n",
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n",
      "75   1238testb Wall   1100754   349968   Newmont Mining Corporation   Newmont Goldcorp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231it [00:01, 135.84it/s]\n",
      "100%|██████████| 231/231 [00:00<00:00, 4652.19it/s]\n"
     ]
    }
   ],
   "source": [
    "splits = ['train.jsonl', 'dev.jsonl', 'test.jsonl']\n",
    "\n",
    "inpath = f'AIDA-YAGO2_longformer/'\n",
    "outpath = f'AIDA-YAGO2_longformer/tokenized/'\n",
    "for split in splits:\n",
    "    examples = reindex(inpath, split)\n",
    "    with open(outpath+split, 'w') as wf:\n",
    "        for example in tqdm(examples):\n",
    "            b = wf.write(json.dumps(example) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/page2id.json', 'w') as f:\n",
    "    json.dump(page2id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
